爬虫流程分析：
	1.先通过指定的url下载json数据
	
	2.json数据下载完之后，遍历json数据
	
	3.使用循环把每个url提取出来
	
		(在跳转之前一定要先得到url)
		1.跳转到url的详细页面
		
		（下载之前要确保已经跳转到了指定页面）
		2.下载详细页面的HTML文件
		
		（提取页面之前要保证已经下载好了页面）
		3.文件下载完成后，提取页面中需要的信息
		
		4.提取完成后保存到数据库
		
	4.开始下一次循环。


app.js结构简介
	1.getUrlStart（）爬取URL的程序的入口，在这个函数中设置每 200ms 调用一次 getUrl 函数爬取URL
	
	2.getDataStart（）爬取具体数据的程序的入口，在这个函数中设置每 150ms 调用一次 getHtml 函数爬取具体数据
	
	3.getUrl（）实现如何爬取URL的函数
	
	4.getHtml（）实现如何通过URL去爬取具体页面数据的函数，这个函数会爬取到特定URL的HTML文件下载下来，在调用getData
		爬取具体数据
		
	5.getData（）在获得特定URL的HTML文件之后，使用该函数爬取需要的数据



如何开始：
1.打开你电脑上面的MongoDB，开启MongoDB服务器

2.然后重新打开一个CMD cd到现在这个文件夹所在的目录

3.执行 cnpm install 命令下载第三方插件

4.更改 app.js 文件中，MongoDB的连接地址（var url = 'mongodb://localhost:27017/2018movies';），
把后面的那个 2018movies 改成你要爬的那个年代

5.更改完成之后，在你爬虫的时候会自动新建一个这个名字的数据库，然后所有的数据都保存在数据库的集合中

6.在app.get中先用第一个函数爬取年份的URL 设置好了之后再命令行中输入node app运行程序

7.爬取年份的URL完了之后再把第一个函数注释，运行第二个函数爬取数据

注意：运行文件之后看到 CMD命令行中  数据库连接上了之后，再用浏览器访问 localhost:8080 这个地址开始爬虫

因为我把函数的调用放在app.get("/")里面，为了实现在爬虫之前数据库已经连接上了

注意：一般用第一个函数爬取URL的时候不会报错，但是使用第二个函数爬取具体的数据时，一般会报两个错误
	1.HTML抓取失败 - Error: Not Found 这个是因为当前爬取的URL 无效
	2.HTML抓取失败 - Error: Aborted 这个是连接超时了，在网络不好的情况下会经常报错，所以最好在网络好的时候爬
	对于第二种报错可以设置 getDataStart 函数中的 Interval = 150 //设置爬虫间隔时间 把时间设置长一点，但是太长了会爬很久